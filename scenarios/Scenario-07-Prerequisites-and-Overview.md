# Scenario 07: Multi-Standby Setup - Complete Guide

**Date:** November 17, 2025  
**Duration:** 35-40 minutes  
**Difficulty:** Advanced

---

## üéØ What This Scenario Tests

### The Big Question:
**"Can PostgreSQL handle multiple standby servers receiving replication from one primary?"**

### Real-World Context:

**As a MySQL DBA, you know:**
- MySQL Master ‚Üí Multiple Replicas (1 master, N replicas)
- Each replica connects independently
- Replicas can lag differently
- Used for read scaling, DR, reporting

**In PostgreSQL:** Same concept, but called "standbys" instead of "replicas"

**Real-world use cases:**

**1. Read Scaling:**
```
Application Load Balancer
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì         ‚Üì         ‚Üì
 Read1     Read2     Read3
(Standby1) (Standby2) (Standby3)
```

**2. Geographic Distribution:**
```
Primary (US-East)
    ‚Üì
    ‚îú‚îÄ‚îÄ‚Üí Standby1 (US-West)  ‚Üê Low latency for West Coast users
    ‚îú‚îÄ‚îÄ‚Üí Standby2 (EU)       ‚Üê Low latency for European users
    ‚îî‚îÄ‚îÄ‚Üí Standby3 (Asia)     ‚Üê Low latency for Asian users
```

**3. Workload Isolation:**
```
Primary (OLTP workload - fast queries)
    ‚Üì
    ‚îú‚îÄ‚îÄ‚Üí Standby1 (Reporting - heavy analytics)
    ‚îú‚îÄ‚îÄ‚Üí Standby2 (ETL - batch data exports)
    ‚îî‚îÄ‚îÄ‚Üí Standby3 (Development - testing queries)
```

**4. High Availability:**
```
Primary
    ‚Üì
    ‚îú‚îÄ‚îÄ‚Üí Standby1 (Hot failover candidate)
    ‚îî‚îÄ‚îÄ‚Üí Standby2 (Warm backup)

If Primary fails:
  ‚Üí Promote Standby1 (seconds)
  ‚Üí Standby2 still available as backup
```

---

## üß™ What We'll Do

### Architecture We'll Build:

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  PRIMARY        ‚îÇ
                    ‚îÇ  Port: 5432     ‚îÇ
                    ‚îÇ  (Read/Write)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                 WAL Stream  ‚îÇ  WAL Stream
                (Async)      ‚îÇ  (Async)
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚Üì                 ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  STANDBY1        ‚îÇ  ‚îÇ  STANDBY2        ‚îÇ
        ‚îÇ  Port: 5433      ‚îÇ  ‚îÇ  Port: 5434      ‚îÇ
        ‚îÇ  (Read-Only)     ‚îÇ  ‚îÇ  (Read-Only)     ‚îÇ
        ‚îÇ  Replication     ‚îÇ  ‚îÇ  Replication     ‚îÇ
        ‚îÇ  Slot: standby   ‚îÇ  ‚îÇ  Slot: standby2  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        
Both standbys:
  ‚Ä¢ Receive same WAL from PRIMARY
  ‚Ä¢ Can lag independently
  ‚Ä¢ Can serve read queries
  ‚Ä¢ Can be promoted to primary if needed
```

### Test Flow:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 1: SETUP SECOND STANDBY (15 min)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Add standby2 to docker-compose.yml                    ‚îÇ
‚îÇ ‚Ä¢ Create replication slot: standby2_slot                ‚îÇ
‚îÇ ‚Ä¢ Take base backup using pg_basebackup                  ‚îÇ
‚îÇ ‚Ä¢ Configure standby2 (standby.signal, configs)          ‚îÇ
‚îÇ ‚Ä¢ Start standby2 container                              ‚îÇ
‚îÇ ‚Ä¢ Verify connection and streaming                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 2: VERIFY REPLICATION (5 min)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Check pg_stat_replication (should show 2 rows)        ‚îÇ
‚îÇ ‚Ä¢ Verify both standbys in recovery mode                 ‚îÇ
‚îÇ ‚Ä¢ Check replication slots (2 active)                    ‚îÇ
‚îÇ ‚Ä¢ Verify initial data sync (row counts match)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 3: TEST WRITE REPLICATION (10 min)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Insert 10,000 rows on PRIMARY                         ‚îÇ
‚îÇ ‚Ä¢ Monitor lag on BOTH standbys                          ‚îÇ
‚îÇ ‚Ä¢ Verify both received all rows                         ‚îÇ
‚îÇ ‚Ä¢ Compare lag between standby1 and standby2             ‚îÇ
‚îÇ ‚Ä¢ Check if lag differs (independent catch-up)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PHASE 4: READ LOAD DISTRIBUTION (5 min)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Run read query on standby1                            ‚îÇ
‚îÇ ‚Ä¢ Run read query on standby2                            ‚îÇ
‚îÇ ‚Ä¢ Compare performance                                   ‚îÇ
‚îÇ ‚Ä¢ Test round-robin read distribution                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîç What Will Happen?

### 1. PRIMARY Sends WAL to Multiple Standbys

**Single WAL Stream ‚Üí Multiple Recipients:**

```
PRIMARY generates WAL:
  INSERT INTO orders VALUES (...)
  ‚Üì
  WAL record created: LSN 0/E000000
  ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ PRIMARY sends same WAL to:  ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ ‚Üí Standby1 (connection 1)   ‚îÇ
  ‚îÇ ‚Üí Standby2 (connection 2)   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**MySQL Equivalent:**
```
Master generates binary log:
  INSERT INTO orders VALUES (...)
  ‚Üì
  Binlog event written
  ‚Üì
  Master sends same event to:
  ‚Üí Replica1 (connection 1)
  ‚Üí Replica2 (connection 2)
```

### 2. Independent Replication Connections

**Each standby has its own:**
- TCP connection to primary
- Replication slot (tracks position)
- WAL receiver process
- WAL replay process
- Lag metrics (independent)

```
STANDBY1:
  Connection: Primary:5432 ‚Üí Standby1:5433
  Process: walreceiver (PID 1234)
  Position: LSN 0/E000000
  Lag: 0 bytes

STANDBY2:
  Connection: Primary:5432 ‚Üí Standby2:5434
  Process: walreceiver (PID 5678)
  Position: LSN 0/DFFFFF0 (slightly behind!)
  Lag: 16 KB
```

**They can lag DIFFERENTLY!**
- Standby1 might be faster (better disk)
- Standby2 might lag (slower network, busy with queries)

### 3. Resource Impact on PRIMARY

**With 1 standby:**
```
PRIMARY CPU:
  ‚Ä¢ Normal workload: 20%
  ‚Ä¢ WAL sender process: 2%
  Total: 22%

PRIMARY Network:
  ‚Ä¢ WAL streaming: 1 MB/sec (to standby1)
```

**With 2 standbys:**
```
PRIMARY CPU:
  ‚Ä¢ Normal workload: 20%
  ‚Ä¢ WAL sender 1: 2%
  ‚Ä¢ WAL sender 2: 2%
  Total: 24%

PRIMARY Network:
  ‚Ä¢ WAL streaming: 2 MB/sec (1 MB to each standby)
```

**Impact:** Minimal! Each standby adds ~2% CPU and proportional network bandwidth.

**With 10 standbys:** Would add ~20% CPU and 10 MB/sec network.

### 4. Read Load Distribution

**Before (1 standby):**
```
Application sends 1000 queries/sec
  ‚Üì
All 1000 ‚Üí Standby1 (overloaded!)
```

**After (2 standbys):**
```
Application sends 1000 queries/sec
  ‚Üì
Load Balancer:
  ‚Ä¢ 500 queries ‚Üí Standby1
  ‚Ä¢ 500 queries ‚Üí Standby2
  
Result: Each handles 50% load!
```

**Benefits:**
- Lower CPU per standby
- Faster query response
- Better resource utilization
- Can handle more total read traffic

---

## üìã Prerequisites Checklist

### ‚úÖ 1. Scenario 06 Completed

**Why:** Need stable replication with current standby

**Check:**
- Previous scenarios 01-06 completed ‚úì
- Understand replication slots
- Understand pg_basebackup
- Understand streaming replication

**Status:** Should be complete from previous work ‚úì

---

### ‚úÖ 2. Current Replication Healthy

**Why:** Start from known good state

**Check:**
```bash
docker exec postgres-primary psql -U postgres -c "
SELECT application_name, state, sync_state 
FROM pg_stat_replication;"
```

**Expected:**
```
application_name | state     | sync_state
-----------------+-----------+------------
walreceiver      | streaming | async
```

**Must verify:**
- ‚úÖ Standby1 currently connected
- ‚úÖ State = streaming
- ‚úÖ No lag or minimal lag

---

### ‚úÖ 3. Sufficient Disk Space

**Why:** Need space for second standby data directory

**Check:**
```bash
df -h
```

**Requirements:**
```
Current database size: ~100 MB (estimated)
Second standby needs: ~100 MB for initial copy
WAL space: ~50 MB (for both standbys)
Total needed: ~150 MB free space

Recommended: 1 GB free space for safety
```

**Docker volumes:**
- Standby2 will need its own volume
- Will be created automatically by Docker

---

### ‚úÖ 4. Docker Resources Available

**Why:** Running 3 PostgreSQL containers simultaneously

**Check:**
```bash
docker stats --no-stream
```

**Current usage:**
```
postgres-primary:  ~200 MB RAM
postgres-standby:  ~200 MB RAM
```

**After adding standby2:**
```
postgres-primary:  ~200 MB RAM
postgres-standby:  ~200 MB RAM (standby1)
postgres-standby2: ~200 MB RAM (new!)
Total: ~600 MB RAM
```

**Requirements:**
- RAM available: 1 GB+ free
- CPU: Not a bottleneck (3 idle containers minimal)

---

### ‚úÖ 5. Port 5434 Available

**Why:** Standby2 will use port 5434

**Check:**
```bash
lsof -i :5434
# or
netstat -an | grep 5434
```

**Expected:** No output (port available)

**If port in use:**
- Stop other application using it
- Or choose different port (modify docker-compose.yml)

**Port mapping:**
```
PRIMARY:   localhost:5432 ‚Üí container:5432
STANDBY1:  localhost:5433 ‚Üí container:5432
STANDBY2:  localhost:5434 ‚Üí container:5432 (new!)
```

---

### ‚úÖ 6. Baseline Metrics Recorded

**Why:** Compare before/after adding second standby

**Check:**
```bash
# Current replication status:
docker exec postgres-primary psql -U postgres -c "
SELECT 
    COUNT(*) as standby_count,
    pg_current_wal_lsn() as current_lsn
FROM pg_stat_replication;"

# Current resource usage:
docker stats --no-stream postgres-primary
```

**Record:**
- Current standby count: 1
- Current WAL position: _____________
- Primary CPU: _____________%
- Primary memory: _____________

---

### ‚úÖ 7. Understanding of pg_basebackup

**Why:** Will use pg_basebackup to initialize standby2

**Concepts to understand:**
- `pg_basebackup` copies entire data directory
- Must run while primary is running (online backup)
- Creates consistent snapshot
- Includes all databases, users, tables
- Does NOT include pg_wal/ directory (WAL files)

**Command we'll use:**
```bash
pg_basebackup -h primary -U replicator -D /data -Fp -Xs -P -R
```

**Parameters:**
- `-h primary` = Connect to primary server
- `-U replicator` = Use replicator user
- `-D /data` = Output directory
- `-Fp` = Plain format (not tar)
- `-Xs` = Stream WAL during backup
- `-P` = Show progress
- `-R` = Create standby.signal and configs automatically

**MySQL Equivalent:**
```bash
# MySQL backup for new replica:
mysqldump --all-databases --master-data=2 > backup.sql
# or
xtrabackup --backup --target-dir=/backup
```

---

## üéì Key Concepts to Understand

### 1. Multi-Standby Topology

**Star Topology (What we're building):**
```
        PRIMARY
       /   |   \
      /    |    \
   SB1   SB2   SB3

Advantages:
  ‚úì Simple to understand
  ‚úì Each standby gets WAL directly from primary
  ‚úì Low latency (one hop)
  
Disadvantages:
  ‚úó Primary handles all WAL sending (N connections)
  ‚úó Network bandwidth from primary √ó N
```

**Cascading Topology (Alternative):**
```
    PRIMARY
       ‚Üì
     SB1
    /   \
  SB2   SB3

Advantages:
  ‚úì Reduces load on primary (only 1 connection)
  ‚úì Reduces primary network bandwidth
  ‚úì Scales better (100s of standbys possible)
  
Disadvantages:
  ‚úó Higher latency (two hops: Primary‚ÜíSB1‚ÜíSB2)
  ‚úó SB1 becomes bottleneck
  ‚úó If SB1 fails, SB2 and SB3 stop receiving WAL
```

**MySQL Comparison:**
```
MySQL Multi-Source Replication:
  Master ‚Üí Replica1
        ‚Üí Replica2
        ‚Üí Replica3

MySQL Chain Replication:
  Master ‚Üí Replica1 ‚Üí Replica2
```

---

### 2. Replication Slots for Each Standby

**Why each standby needs its own slot:**

```
Without slots:
  PRIMARY: "I'm at LSN 0/F000000"
  Standby1: "I need LSN 0/E000000" (lagging)
  PRIMARY: "Sorry, I deleted that WAL already!" ‚ùå
  
With slots:
  Standby1 slot: restart_lsn = 0/E000000
  Standby2 slot: restart_lsn = 0/E500000
  PRIMARY: "I'll keep WAL from 0/E000000 (earliest slot)" ‚úì
```

**Slot tracking:**
```sql
SELECT 
    slot_name,
    restart_lsn,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS retained
FROM pg_replication_slots;

Result:
slot_name    | restart_lsn | retained
-------------+-------------+----------
standby_slot | 0/E000000   | 16 MB    ‚Üê Standby1 lagging
standby2_slot| 0/EF00000   | 1 MB     ‚Üê Standby2 almost caught up
```

**PRIMARY retains WAL from EARLIEST slot** (0/E000000 in this case)

**MySQL Equivalent:**
```sql
-- MySQL doesn't have replication slots
-- Must manually configure:
SET GLOBAL binlog_expire_logs_seconds = 259200; -- 3 days

-- Risk: If replica offline > 3 days, binlogs purged!
-- PostgreSQL slots = automatic retention ‚úì
```

---

### 3. Independent Lag Metrics

**Each standby tracks lag separately:**

```sql
SELECT 
    application_name,
    pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS lag_bytes,
    replay_lag
FROM pg_stat_replication;

Result:
application_name | lag_bytes | replay_lag
-----------------+-----------+------------
walreceiver      | 0         | 00:00:00    ‚Üê Standby1 (fast)
walreceiver2     | 16384     | 00:00:02    ‚Üê Standby2 (lagging)
```

**Why lag differs:**

**Hardware differences:**
- Standby1: SSD (fast replay)
- Standby2: HDD (slower replay)

**Load differences:**
- Standby1: Idle (only replication)
- Standby2: Running heavy query (CPU busy, slow replay)

**Network differences:**
- Standby1: Same datacenter (1ms latency)
- Standby2: Remote datacenter (50ms latency)

**This is NORMAL and EXPECTED!**

---

### 4. Read Load Balancing

**Simple Round-Robin:**
```
Query 1 ‚Üí Standby1
Query 2 ‚Üí Standby2
Query 3 ‚Üí Standby1
Query 4 ‚Üí Standby2
...
```

**Least-Connections:**
```
Standby1: 50 active connections ‚Üí Send query to Standby2
Standby2: 20 active connections ‚Üí Send query here ‚úì
```

**Lag-Aware:**
```
Standby1: 0 bytes lag ‚Üí Send query here ‚úì
Standby2: 10 MB lag   ‚Üí Skip (too far behind)
```

**Geographic:**
```
User in US-East  ‚Üí Route to Standby1 (US-East)
User in US-West  ‚Üí Route to Standby2 (US-West)
User in Europe   ‚Üí Route to Standby3 (EU)
```

**Application-level (connection string):**
```python
# Python example:
import random
standbys = ['localhost:5433', 'localhost:5434']
conn = psycopg2.connect(host=random.choice(standbys), ...)
```

**Load balancer (HAProxy, pgpool-II):**
```
Application ‚Üí HAProxy (localhost:5432)
              ‚Üì
              ‚îú‚îÄ‚Üí Standby1 (50% traffic)
              ‚îî‚îÄ‚Üí Standby2 (50% traffic)
```

---

### 5. Failover with Multiple Standbys

**Scenario: Primary fails**

```
Before:
  PRIMARY (failed!) √ó
     ‚Üì
     ‚îú‚îÄ‚îÄ‚Üí Standby1 (0 bytes lag)
     ‚îî‚îÄ‚îÄ‚Üí Standby2 (100 KB lag)

Decision: Promote Standby1 (less lag = more data)

After promotion:
  PRIMARY (was Standby1) ‚úì
     ‚Üì
     ‚îî‚îÄ‚îÄ‚Üí Standby2 (reconnects to new primary)

Standby2 must:
  1. Follow new primary timeline
  2. Update primary_conninfo to point to Standby1
  3. Continue replication
```

**PostgreSQL timeline handling:**
```
Timeline 1: Original primary (before failover)
Timeline 2: Standby1 promoted (after failover)

Standby2 will follow timeline 2 automatically!
```

**MySQL Comparison:**
```sql
-- MySQL failover with multiple replicas:
-- Promote Replica1:
STOP SLAVE;
RESET MASTER;
-- Replica1 is now master

-- Reconfigure Replica2:
STOP SLAVE;
CHANGE MASTER TO MASTER_HOST='replica1', ...;
START SLAVE;
```

---

## üö® Important Notes

### 1. Resource Requirements Scale Linearly

**With N standbys:**
- PRIMARY CPU: +2% per standby (~24% with 2 standbys)
- PRIMARY Network: +(WAL rate) per standby (~2 MB/sec per standby)
- PRIMARY Memory: Minimal increase (~10 MB per connection)

**Practical limits:**
- **Small deployments:** 2-5 standbys (typical)
- **Medium deployments:** 5-10 standbys (requires monitoring)
- **Large deployments:** 10-20 standbys (use cascading)
- **Extreme:** 100+ standbys (use cascading replication)

---

### 2. WAL Retention Critical

**With multiple standbys:**
- PRIMARY retains WAL from SLOWEST standby
- If one standby lags badly, WAL accumulates
- Disk can fill up!

**Monitor:**
```sql
SELECT 
    slot_name,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS retained
FROM pg_replication_slots
ORDER BY restart_lsn;

-- If any standby shows > 1 GB retained: INVESTIGATE!
```

**Fix slow standby:**
- Upgrade hardware
- Reduce query load
- Move to cascading replication
- Drop slot and rebuild (last resort)

---

### 3. All Standbys Are Async (For Now)

**In this scenario:**
- Both standbys use async replication
- Primary doesn't wait for either standby
- Fast commits, but data loss risk if primary crashes

**In Scenario 08:**
- We'll configure synchronous replication
- Primary waits for at least 1 standby
- Slower commits, but zero data loss guarantee

---

### 4. Network Bandwidth Planning

**Calculate bandwidth needed:**
```
Average WAL rate: 10 MB/sec
Number of standbys: 2
Total bandwidth: 10 √ó 2 = 20 MB/sec = 160 Mbps

For 10 standbys: 100 MB/sec = 800 Mbps
For 100 standbys: 1 GB/sec = 8 Gbps ‚Üê Need cascading!
```

**Our Docker setup:** Localhost (no bandwidth limits)

**Production:** Ensure network can handle N √ó WAL rate

---

## üìä Success Criteria

After completing Scenario 07, you should see:

### ‚úÖ 1. Second Standby Running
- postgres-standby2 container up
- Listening on port 5434
- Data directory initialized

### ‚úÖ 2. Both Standbys Connected
```sql
SELECT COUNT(*) FROM pg_stat_replication;
-- Should return: 2
```

### ‚úÖ 3. Both Slots Active
```sql
SELECT COUNT(*) FROM pg_replication_slots WHERE active = true;
-- Should return: 2
```

### ‚úÖ 4. Data Replicated to Both
```sql
-- PRIMARY:
SELECT COUNT(*) FROM orders;
-- STANDBY1:
SELECT COUNT(*) FROM orders;
-- STANDBY2:
SELECT COUNT(*) FROM orders;
-- All should match!
```

### ‚úÖ 5. Both Can Serve Reads
- Connect to standby2:5434
- Run SELECT query successfully
- Verify read-only (INSERT fails)

### ‚úÖ 6. Independent Lag Tracking
- Can monitor each standby separately
- Lag may differ between standbys
- Both eventually catch up

---

## üé¨ Ready to Start!

**Prerequisites understood:**
- ‚úÖ What multi-standby topology is
- ‚úÖ Why each standby needs its own slot
- ‚úÖ How to use pg_basebackup
- ‚úÖ How independent lag works
- ‚úÖ How read load distribution benefits

**What we'll build:**
```
PRIMARY:5432 (Read/Write)
  ‚îú‚îÄ‚îÄ‚Üí STANDBY1:5433 (Read-Only)
  ‚îî‚îÄ‚îÄ‚Üí STANDBY2:5434 (Read-Only)
```

**Next step:** Modify docker-compose.yml to add standby2!

---

*Prerequisites document created: November 17, 2025*  
*Ready for Scenario 07: Multi-Standby Setup execution*
